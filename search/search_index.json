{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>OCTRON is a pipeline built on napari that enables segmentation and tracking of animals in behavioral setups. It helps you to create rich annotation data that can be used to train your own machine learning segmentation models. This enables dense quantification of animal behavior across a wide range of species and video recording conditions. </p> <p>Octron is built on napari, segment anything, yolo and \ud83d\udc9c.</p>    Your browser does not support the video tag.  <p>Installation How to use </p> <p>How to cite</p> <p>Using OCTRON for your project? Please cite this paper to share the word!   \ud83d\udc49Add paper details</p> <p>Attributions</p> <ul> <li>Interface button and icon images were created by user Arkinasi from Noun Project (CC BY 3.0)</li> <li>Logo font: datalegreya</li> <li>OCTRON training is accomplished via ultralytics:  <pre><code>@software{yolo11_ultralytics,\n  author = {Glenn Jocher and Jing Qiu},\n  title = {Ultralytics YOLO11},\n  version = {11.0.0},\n  year = {2024},\n  url = {https://github.com/ultralytics/ultralytics},\n  orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},\n  license = {AGPL-3.0}\n}\n</code></pre></li> <li>OCTRON annotation data is generated via Segment Anything: <pre><code>@article{kirillov2023segany,\n  title={Segment Anything},\n  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\\'a}r, Piotr and Girshick, Ross},\n  journal={arXiv:2304.02643},\n  year={2023}\n}\n</code></pre> <pre><code>@inproceedings{sam_hq,\n    title={Segment Anything in High Quality},\n    author={Ke, Lei and Ye, Mingqiao and Danelljan, Martin and Liu, Yifan and Tai, Yu-Wing and Tang, Chi-Keung and Yu, Fisher},\n    booktitle={NeurIPS},\n    year={2023}\n}  \n</code></pre></li> <li>OCTRON multi-object tracking is achieved via ByteTrack and BoT-SORT: <pre><code>@article{zhang2022bytetrack,\n  title={ByteTrack: Multi-Object Tracking by Associating Every Detection Box},\n  author={Zhang, Yifu and Sun, Peize and Jiang, Yi and Yu, Dongdong and Weng, Fucheng and Yuan, Zehuan and Luo, Ping and Liu, Wenyu and Wang, Xinggang},\n  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},\n  year={2022}\n}\n</code></pre> <pre><code>@article{aharon2022bot,\n  title={BoT-SORT: Robust Associations Multi-Pedestrian Tracking},\n  author={Aharon, Nir and Orfaig, Roy and Bobrovsky, Ben-Zion},\n  journal={arXiv preprint arXiv:2206.14651},\n  year={2022}\n}\n</code></pre></li> </ul>"},{"location":"access-data/","title":"Access output data","text":"<p>To access the data that OCTRON has generated (the coordinates of all your labelled objects for each frame, masks, etc), you can use this tutorial notebook.</p>"},{"location":"analysing/","title":"Analyze (new) videos","text":"<p>Once you have a trained model, you can use it to analyze new videos.</p> <p></p>"},{"location":"analysing/#add-video-files","title":"Add video files","text":"<p>Start by adding the video(s) you want to analyse in the Add video files section</p>"},{"location":"analysing/#create-predictions-from-videos","title":"Create predictions from videos","text":"<p>Next, in the Create predictions from videos section you select how the videos you selected should be analyzed with the following options:</p> <ul> <li>Choose model...: choose which model you want to use from the drop-down menu. You can choose models from different stages of the training, e.g. the model that was saved after x number of epochs, or the best/last one that was saved (recommended).</li> <li> <p>Tracker...: if you have annotated more than one object for a given label (e.g. artemia 1, artemia 2), a tracker needs to be used to help the model determine which is which across frames. Pick the one you prefer in this drop-down menu. If you only have one object per label, click the 1 subject option.</p> Which tracker should I use? <p>Short answer: it probably doesn't matter which one you choose. You can find detailed information from the creators of each tracker here: </p> <ul> <li>ByteTrack </li> <li>BoT-SORT </li> </ul> </li> <li> <p>Videos: select which video you want to analyze first.</p> </li> <li>Polygon sigma (x-y): details</li> <li>Conf. thres. (confidence threshold, 0-1): the confidence threshold that should be used to determine which tracked frames to keep. There will likely be frames where the model is more confident that it has identified the right object than others. If the confidence threshold is set to 0.8 it means the model will only track frames where it is 80% certain that it has correctly identified a given object.</li> <li>View results: select this option if you want OCTRON to automatically open a new window where you can see the result of the analysis once it is complete.</li> <li>Overwrite: select this option if you've previously analysed the selected video and want to replace that analysis,</li> <li>IOU (intersection over union, 0-1): this threshold determines how much objects can overlap and still be considered separate objects. If this value is zero, then objects that have no overlap will be considered to be the same object, i.e. there is only one object.</li> </ul> <p>Click Predict</p> <p>OCTRON will now analyze your video(s) and show its progress via the progress bars (if analyzing multiple videos at once, then the first progress bar displays the progress of a single video while the second bar updates with the completion of each video) along with an estimate for when the analysis will finish.</p>"},{"location":"annotating/","title":"Generate annotation data","text":"<p>To train your model you first need to show it what it should be tracking, i.e. you need to annotate the videos you want the model to train on. This is done in the Generate annotation data tab.</p> <p></p>"},{"location":"annotating/#model-selection","title":"Model selection","text":"<p>Select the model you would like to use for your annotations and click Load model</p> Which model should I choose? <p>Rule of thumb: the larger the model, the more resources (GPU) it demands. SAM2 Base Plus: details  SAM2 Tiny: details  SAM2 Small: details  SAM2 Large: details  SAM2 Large HQ: details</p>"},{"location":"annotating/#create-labels","title":"Create labels","text":"<p>This is where you create the labels for the animals/item/structure you want to track. </p> <ol> <li> <p>In the Type... drop-down menu, select the type of annotation you would like to use to label your structure. There are two types:</p> <ul> <li>Points: often the simplest and fastest. The model will automatically label what it thinks you want to track based on what you left-click on and exclude anything you right-click on. </li> <li>Shapes: recommended for labelling items that are not so easily created with the 'Points' type. Here you make an outline around the item to show the model what it should label.</li> </ul> </li> <li> <p>In the Label... drop-down menu, select Create to open a dialogue box where you can name your label and click Add to add it. </p> <p>Option:</p> <ul> <li>Suffix: add a number here if you want to label multiple instances of the same thing (e.g. you have two LEDs and want to label them separately as LED 1 and LED 2)</li> </ul> </li> <li> <p>Click the Create button to create your label. Two new layers will appear in the left hand section of OCTRON (more on that later).</p> </li> <li> <p>Repeat steps 1-3 until you have all the labels you want.</p> </li> </ol> Removing unwanted labels <p>If you want to remove one of the labels you've created, then click Remove in the Label... drop-down menu and select the label you would like to remove</p> <p>If at some point the model that is helping you annotate the videos is starting to slow down, you can click the Reset button to make it forget what it's learned so far and start fresh.</p>    Your browser does not support the video tag."},{"location":"annotating/#start-annotating","title":"Start annotating","text":"<p>In the bottom left section of OCTRON you have a layer list of all your layers. When you click on a layer you'll get access to its layer controls in the panel directly above.</p> <p>Each label that you created has two layers: one labelled by the layer type you selected (points or shapes) and one called mask. The points/shapes layer is the one you use to make annotations, while the mask layer is where you see the result of your annotations (i.e. what OCTRON has identified as an object based on your annotations). </p> <p>All layers can be toggled visible/invisible by clicking the \ud83d\udc41\ufe0f symbol on that layer.</p> <ol> <li> <p>Decide which frame to annotate first (e.g. the first frame where all the items you want to label are visible)</p> </li> <li> <p>Click the video layer and make any adjustments necessary (e.g. adjust contrast).</p> </li> <li> <p>Click on a points layer and make sure the \u2795 symbol is selected in the layer controls panel. Use your mouse to left-click on the object you want to track - you should see a translucent mask appear covering that object. Right-click on anything that should not be included in that mask. The more clicks you make of both kinds, the more refined the mask becomes. The clicks do not have to be very precise.</p> How to remove unwanted points <p>If you make a mistake and would like to remove a point, click the \u2716\ufe0f symbol in the layer controls and left-click on the point you would like to remove to delete it</p> </li> <li> <p>Click on a shapes layer and select the type of shape you want to use in the layer controls, e.g. a square. Left-click and drag the shape around the object you want to label - OCTRON will automatically try to identify the structure you want to label within that square. </p> <p>    Your browser does not support the video tag. </p> </li> <li> <p>You can refine a shapes layer by using the tools shown in that layer's layer controls (e.g. remove/add/adjust points on the shape outline). </p> </li> <li> <p>If you want to switch the annotation type (e.g. from points to shapes), delete the mask layer associated with that annotation type by selecting it and clicking the \ud83d\uddd1\ufe0f symbol (both the mask and points layers will be removed), then add the layer again (step 1 under Create labels)</p> <p>    Your browser does not support the video tag. </p> </li> <li> <p>Once you have annotated all the object you want to track in a single frame, you can get help from OCTRON to annotate the remaining frames (see Batch prediction)</p> </li> </ol>"},{"location":"annotating/#batch-prediction","title":"Batch prediction","text":"<ol> <li> <p>Click \u25b6\ufe0f to predict the next frame. The model you selected under Model selection will now create masks in the following frame, on what it thinks are the same objects as those you've annotated. </p> What to do if the predicted masks look bad <p>If this happens then you need to refine the predicted mask. </p> <ul> <li>If you used the points label, then you just need to add a few more left- and right-clicks on the new frame to helpt the model recognise the object </li> <li>If you used the the shapes label it's often easiest to redraw the shape in the new frame </li> </ul> </li> <li> <p>If you're happy with the prediction, continue clicking \u25b6\ufe0f to see if the predictions continue to look good for the following frames, adjusting the masks if necessary</p> </li> <li> <p>Once the predictions seem to be reliably good, click the 15 frames button to predict 15 frames in a row. Once the predictions are finished, you can go back and adjust the masks if necessary; either individually if there's only one or two frames that are off, or just the first frame where the predictions went wrong and then try predicting 15 frames again from there (the new predictions will overwrite the old ones)</p> </li> <li> <p>When predicting 15 frames in a row works well, then you can start to skip frames to speed up the process, especially if there is very little happening from frame to frame. If at some point you need to return to a previously annotated frame that was several frames away, you can use the timeline control to quickly move between them.</p> <ul> <li>Skip: the number of frames you want to skip before predictions should be made again (this will apply both if you click \u25b6\ufe0f and if you click 15 frames)</li> <li>Timeline control: click Jump to previous or Jump to next to move to the closest preceding/upcoming annotated frame</li> </ul> </li> <li> <p>Continue to predict frames until you reach the end of the video or a decent number of frames have been annotated</p> How do I know how many frames have been annotated? <p>Open the Manage project tab and look for the video you're currently annotating in the Existing data list. The last few characters of the folder and file name are visible in the first two columns, followed by the number of labels in each video, and the total number of frames that have been annotated.</p> <p></p> How many frames should I annotate? <p>This depends on xyz</p> If the predictions become much slower than they were in the beginning <p>This sometimes happens when the model is basing its predictions on a large number of annotated frames. Click the Reset button under Label manager to make it forget what it's learned so far and then create annotations for all your labels in a single frame before you start predicting again.</p> </li> </ol>    Your browser does not support the video tag."},{"location":"create-project/","title":"Create a project","text":""},{"location":"create-project/#start-by-organising-your-project","title":"Start by organising your project","text":"<ol> <li> <p>Decide which videos to use and make sure they are .mp4</p> How do I convert my video files to .mp4? <p>If your videos are not in mp4 format then you can use OCTRON to transcode them!</p> <ol> <li>Drag the folder containing the videos you want to transcode into the centre area of the OCTRON GUI.</li> <li>In the dialogue box that opens, make sure OCTRON is selected and click OK.</li> <li> <p>OCTRON will identify the video files in that folder and let you select which ones to transcode. </p> <p>Options: </p> <ul> <li>Create subfolder: Select this if you want the new .mp4 files to be saved in a subfolder to keep them separate from the originals.</li> <li>CRF (constant rate factor): this value determines the quality of the .mp4 files. Lower values mean higher quality but also larger file size.</li> </ul> </li> <li> <p>Click OK and open your terminal window to check on progress. As the files are transcoded you will see them pop up in your folder (or a mp4_transcoded subfolder within your forlder)</p> </li> </ol> <p>     Your browser does not support the video tag. </p> </li> <li> <p>Create a folder for your project</p> Project folder tip <p>Within your project folder, create two subfolders:</p> <ul> <li>train: save the videos that you want to train the model on in this folder. The number of videos you'll need for training depends on what you want to track, the quality of the data, recording duration, etc. Recommendation: start with a handful of videos and add more later if the training results are not good enough.</li> <li>test: save the videos that you want to test the model on here (i.e. videos that the model has not been trained on). These should be new videos, but similar to the training videos in terms of image characteristics.</li> </ul> </li> <li> <p>Start the OCTRON Gui</p> </li> <li> <p>In the Manage project tab, click Choose under Project folder and navigate to the folder you created in step 2</p> </li> </ol>    Your browser does not support the video tag."},{"location":"create-project/#add-new-video-file","title":"Add new video file","text":"<p>Drag one of the videos you want to train the model on into the Add new video file section under the Manage project tab. If the video is not located in the project folder, a dialogue box will pop up asking if you'd like to save a copy of the video in the project folder. It is highly recommended to click Yes to ensure all project related files are kept together. You will find the copied video in a subfolder called videos within your project folder.</p>    Your browser does not support the video tag."},{"location":"create-project/#existing-data","title":"Existing data","text":"<p>Once you start annotating, this section will list the number of frames that you have annotated in each video as well as the number of labels. </p> <p></p>"},{"location":"gui/","title":"Using the GUI","text":""},{"location":"gui/#opening-the-gui","title":"Opening the GUI","text":"<ol> <li>Activate the new environment:     <pre><code>conda activate octron\n</code></pre></li> <li>Start the GUI:     <pre><code>octron-gui\n</code></pre>     ... and enjoy!      On first start this will take a long time to load, since all available models are also downloaded. Subsequent startups will be much quicker. </li> </ol>"},{"location":"gui/#the-gui-explained","title":"The GUI explained","text":"<p>In the GUI there are four main panels:</p> <ul> <li>Project navigation: this is where you navigate through the different steps of the process, from opening your project (Manage project tab), annotate videos (Generate annotation data tab), train a model on your annotated videos (Train model tab), and use your trained model to analyze new videos (Analyze (new) videos tab)</li> <li>Video panel: the video you are annotating will show up here</li> <li>Layer list: the annotations you create will be shown as individual layers in this section</li> <li>Layer controls: the tools to create annotations will be found here once you've selected a specific layer in the layer list</li> </ul> <p></p>"},{"location":"installation/","title":"Installation","text":"<p>Important Update Information</p> <p>Just a heads up that there will be changes accumulating in the repo over these days.  So the first thing that you should do before playing with OCTRON (after you installed it successfully once, see below), should be:</p> <ol> <li>Pull latest changes from main (in the github desktop app for example)</li> <li>In your terminal, browse to your cloned repository folder on disk and then</li> <li><code>pip install . -U</code> for installing the new code and to update existing packages.</li> </ol> <p>If you ever mess up completely, do not despair! You can trash everything with:</p> <ul> <li><code>conda deactivate</code> and then</li> <li><code>conda env remove --name octron --yes</code>.</li> </ul> <p>Then you have to start with the conda .yml again (see steps explained below).</p>"},{"location":"installation/#follow-these-steps-to-install-octron","title":"Follow these steps to install OCTRON:","text":"<ol> <li> <p>Make sure ffmpeg is installed on the system. Some packages rely on it.</p> <ul> <li><code>ffmpeg -version</code> If the command fails for some reason, make sure you install ffmpeg first</li> </ul> Installing ffmpeg WindowsMacOSLinux <p>On MacOS you can use homebrew and <code>brew install ffmpeg</code></p> <p>You know what to do (:</p> </li> <li> <p>Download miniconda. </p> <ul> <li>Open your web browser and go to the official Miniconda download page: Miniconda Download. </li> <li>Download and execute the installer for your operating system (Windows, macOS, or Linux). </li> <li>Restart your terminal.</li> </ul> </li> <li> <p>Clone the OCTRON-GUI repository and in a terminal (CMD on Windows) browse to the folder that you cloned it to (<code>cd \"YOUR/CLONED/FOLDER\"</code>)</p> </li> <li> <p>Create a new Conda environment called \"octron\" with Python version 3.11:     <pre><code>conda env create -f environment.yaml\n</code></pre></p> <p>CUDA Users</p> <p>If you have a CUDA compatible graphics card in your computer, do instead:</p> <pre><code>conda env create -f environment_cuda.yaml\n</code></pre> <p>This will install the right PyTorch version automatically on Windows systems.</p> </li> <li> <p>Activate the new environment:     <pre><code>conda activate octron\n</code></pre></p> </li> <li>Check the accessibility of GPU resources on your computer:     <pre><code>python test_gpu.py\n</code></pre>     This should show your graphics card, if it is correctly installed and accessible by PyTorch. If this fails, you should correct this first, since OCTRON will not engage your GPU otherwise (and thus be much slower).</li> </ol>"},{"location":"installation/#step-1-download-ffmpeg","title":"Step 1: Download FFmpeg","text":"<ol> <li>Open your web browser and go to the official FFmpeg download page: FFmpeg Download.</li> <li>Under the \"Get packages &amp; executable files\" section, click on the Windows logo.</li> <li>You will be redirected to a page with various builds. Click on the link for the \"Windows builds from gyan.dev\".</li> <li>On the gyan.dev page, scroll down to the \"Release builds\" section and download the \"ffmpeg-release-essentials.zip\" file.</li> </ol>"},{"location":"installation/#step-2-extract-the-ffmpeg-zip-file","title":"Step 2: Extract the FFmpeg Zip File","text":"<ol> <li>Once the download is complete, navigate to the folder where the zip file was downloaded.</li> <li>Right-click on the <code>ffmpeg-release-essentials.zip</code> file and select \"Extract All...\".</li> <li>Choose a destination folder to extract the files to (e.g., <code>C:\\ffmpeg</code>) and click \"Extract\".</li> </ol>"},{"location":"installation/#step-3-add-ffmpeg-to-the-system-path","title":"Step 3: Add FFmpeg to the System Path","text":"<ol> <li>Open the extracted folder (e.g., <code>C:\\ffmpeg</code>) and navigate to the <code>bin</code> directory.</li> <li> <p>Copy the path to the <code>bin</code> directory (e.g., <code>C:\\ffmpeg\\bin</code>).</p> </li> <li> <p>Open the Start menu, search for \"Environment Variables\", and select \"Edit the system environment variables\".</p> </li> <li>In the System Properties window, click on the \"Environment Variables...\" button.</li> <li>In the Environment Variables window, find the \"Path\" variable under the \"System variables\" section and select it. Click \"Edit...\".</li> <li>In the Edit Environment Variable window, click \"New\" and paste the path to the <code>bin</code> directory (e.g., <code>C:\\ffmpeg\\bin</code>). Click \"OK\" to close all windows.</li> </ol>"},{"location":"installation/#step-4-verify-the-installation","title":"Step 4: Verify the Installation","text":"<ol> <li>Open the Command Prompt by pressing <code>Win + R</code>, typing <code>cmd</code>, and pressing <code>Enter</code>.</li> <li>In the Command Prompt, type <code>ffmpeg -version</code> and press <code>Enter</code>.</li> <li>If FFmpeg is installed correctly, you should see the version information for FFmpeg.</li> </ol>"},{"location":"installation/#step-5-use-ffmpeg","title":"Step 5: Use FFmpeg","text":"<p>You can now use FFmpeg from the Command Prompt or any other terminal on your Windows system.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during the installation process, make sure to:</p> <ul> <li>Double-check that the path to the <code>bin</code> directory is correct.</li> <li>Ensure that the path is added to the \"System variables\" section, not the \"User variables\" section.</li> <li>Restart your Command Prompt or computer to apply the changes.</li> </ul>"},{"location":"training/","title":"Train model","text":"<p>Once you have a decent number of annotated frames, you are ready to train your model. This is done in the Train model tab.</p> <p></p>"},{"location":"training/#generate-training-data","title":"Generate training data","text":"<p>OCTRON needs to generate data to train the model on, i.e. it takes you annotations and splits them into a training dataset and a testing dataset. This enables it to evaluate how well the training is going by comparing its predictions against the ground truth. First, consider these options:</p> <ul> <li>Prune: select this if there are frames in which it is likely that not all of your objects were annotated despite them all being present. By selecting this option OCTRON will 'prune' the annotated frames so that only those where all labels are present are used. Otherwise you will be counteracting the training (the model will think that if one object isn't annotated in a certain frame, but the other objects are, this means the un-annotated object isn't there)</li> <li>Overwrite: if you've generated a training dataset before, selecting this option will overwrite the existing one (recommended)</li> </ul> <p>Once you click Generate, you can observe the progress in the two progress bars:</p> <ul> <li>label: details</li> <li>label and split: details</li> </ul>"},{"location":"training/#train","title":"Train","text":"<p>Once the training data has been generated, OCTRON is ready to train your model. There are a few settings to consider:</p> <ul> <li> <p>Choose model: choose which model to use</p> Which model should I choose? <ul> <li>YOLO11m-seg: details </li> <li>YOLO11l-seg: details </li> <li>YOLO11x-seg: details</li> </ul> </li> <li> <p>Img. size: choose which image size OCTRON should work with</p> Which image size should I choose? <ul> <li>x: details </li> <li>y: details </li> </ul> </li> <li> <p>Epochs: decide how many epochs the model should train for. The higher the number, the longer the training will take, but if no significant improvement is detected across x number of epochs then the training will automatically stop</p> </li> <li> <p>Save period: decide how often (in number of ephocs) to save the training results</p> </li> <li> <p>Resume: if you've previously started training a model but had to abort for some reason, you can continue from where the training stopped by selecting this option [this feature has not been implemented yet]</p> </li> <li>Overwrite: if you've previously trained a model and want to replace it, select this option</li> <li>Tensorboard: select this if you want to follow the training progress live in your browser. Once you click Train, a new browser tab should open automatically, but if it does not you can copy the link that will appear in your terminal window and open it manually</li> </ul> <p>Click Train</p> <p>While the model is training you can track its progress in the terminal window (and with graphs in your browser if you selected the Tensorboard option). Once OCTRON has finished one epoch, it will provide an estimate of how long the total training will take, based on how long the first epoch took to complete and how many epochs you've told it to train for. </p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Having problems? If you can't find a solution in the documentation or in the FAQs below, check if someone else has had the same problem on the repository issues page. If not, add it and we'll try to help! </p>"},{"location":"troubleshooting/#frequency-asked-questions","title":"Frequency asked questions","text":"My GPU isn't engaged - what do I do? <p>If everything is slow and you notice that everything is running on your CPU instead of your GPU and you get this error:  <pre><code>\u201cRuntimeError: could not run \u2018torchvision::nms\u2019 with arguments from the \u2018CUDA\u2019 backend.\" \n</code></pre> then that means torchvision was installed without cuda support. Running this should solve the problem: <pre><code>pip install --force-reinstall torchvision==0.21.0 --index-url https://download.pytorch.org/whl/cu126\n</code></pre></p> Some other issue <p>What to do</p>"}]}